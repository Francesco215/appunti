%\documentclass[11pt,fleqn]{book}%
%\documentclass{exam}%
\documentclass[11pt,a4paper]{report}
\date{{\LARGE Febbraio 2018}}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[11pt]{moresize}

\font\myfont=cmr12 at 40pt
\title{{\myfont Algebra Continua}}

\author{{\Huge Francesco Sacco}\\ \\ \\
		\includegraphics[scale=0.6]{Immagini/cherubino.eps}\\}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}

\newcommand{\vettore}[1]{\mathbf{#1}}
\newcommand{\vettorec}[1]{\textrm{#1}}
\newcommand{\pscal}[2]{\langle \vettore{#1},\vettore{#2}\rangle}

\theoremstyle{definition}
\newtheorem{definizione}{Definizione}

\theoremstyle{plain}
\newtheorem{teorema}{Teorema}

\theoremstyle{plain}
\newtheorem{esempio}{Esempio}


\begin{document}
	\maketitle
	\tableofcontents
	\chapter*{Prefazione}
		In questo articolo non faccio altro che prendere qualcosa di discreto, tipo il numero di dimenzioni di uno spazio vettoriale, e portarlo al continuo. Così facendo è stato possibile raggiungere alcuni risultati noti della matematica come la serie e trasformata di Furiè, alcuni elementi del calcolo variazionale, le equazioni di Eulero-Lagrange e altro ancora.\newline
		Il mio obbiettivo è cercare di spiegare tutti gli argomenti trattati partendo dai risultati di algebra lineare e analisi 1 e 2, quindi prima a o poi scriverò quelle quattro fissarie che servono dell'algebra multinineare per il secondo capitolo.\newline
		Ovviamente non sarò formale, quindi Ischia non ci scassare la minchia, e farò parecchi errori grammaticali, quindi Silvio non ci scassare la minchia.\newline
		Visto che detesto le cose prolisse ho deciso di mettere alcuni dettagli che secondo me sono superflui o ovvi a piè di pagina, quindi credo che sia meglio non leggerli affatto, ma li ho messi per completezza.\newline
		In ogni caso dalla regia mi comunicano che è scritto con i piedi, ma non so come rimediare, quindi qualche consiglio sarebbe gradito.\newline
		Un ringraziamento speciale va a Tiziano Amato perchè mi ha chiesto di scrivergli un ringraziamento da qualche parte.\newline

		Per quanto riguarda la notazione indicherò i vettori così $\vettore v$, mentre le componenti dei vettori così $\vettorec v_i$, o alternativamente $(\vettore v)_i$. Attenzione! $\vettore v_i$ indica il vettore $i$-esimo, non una componente e inoltre per indicare la componente $i$-esima del vettore $j$-esimo farò così $(\vettore v_j)_i$\newline

		Buona lettura.
	
	\chapter{Algebra lineare}
		\section{Dalle matrici discrete a quelle continue}
			Supponiamo di prendere una matrice $M$ $ n \times m$, adesso rendiamo le cellette della matrice sempre più strette mandando $n$ a infinito.
			\begin{equation}
				\begin{vmatrix}
					a_{1,1} & \dots & a_{1,n} \\
					\vdots	&	\ddots&	\vdots	\\
					a_{n,1} & \dots & a_{n,n} 
				\end{vmatrix}
				\longrightarrow
				\begin{vmatrix}
					&\quad & & \\
					&\ &\quad \cdot \, (x,y) & \\
					& & & \\
				\end{vmatrix}
			\end{equation}
			Così facendo non è più possibile indicare un'elemento della matrice con una coppia di numeri interi, ma è possibile indicarli con una coppia di numeri reali $(x,y)$, di  conseguenza la matrice non è più una funzione $M:\{1,\dots,n\}\times\{1,\dots,m\} \rightarrow \rm I\!R$, ma $M:[a,b]\times [c,d]\rightarrow \rm I\!R$.\newline
			Usando lo stesso ragionamento con i vettori abbiamo che la loro versione con $n\rightarrow +\infty$ siano funzioni $\vettore v:[a,b]\rightarrow \rm I\!R$ 
		
		

		\section{Prodotto caso discreto}
		\label{sec:prod-can}
			In algebra lineare un prodotto tra una matrice $M$ e un vettore $\vettore v$ non è altro che un vettore $\vettore w$ che ha come componente $i$-esima il prodotto scalare canonico tra il vettore $\vettore v$ e la $i$-esima riga di $M$, quindi vediamo prima di generalizzare il prodotto scalare.\newline
			Possiamo definire il prodotto scalare in questo modo: prendiamo questa funzione $P:V\times V\rightarrow V$ tale che 
			\begin{equation}
				\vettore v=
				\begin{vmatrix}
					\vettorec v_1\\
					\vdots \\
					\vettorec v_n
				\end{vmatrix}
				\quad \vettore w=
				\begin{vmatrix}
					\vettorec w_1\\
					\vdots \\
					\vettorec w_n
				\end{vmatrix}
				\quad P(\vettore v,\vettore w)=
				\begin{vmatrix}
					\vettorec v_1 \vettorec w_1\\
					\vdots \\
					\vettorec v_n \vettorec  w_n
				\end{vmatrix}
			\end{equation}
			Una volta aver definito questa apparentemente inutile funzione ne definisco un'altra. Prendi un vettore $\vettore v=(\vettorec v_1,\dots,\vettorec v_n)$ e immagina che ogni componente uscisse dal foglio con un'altezza pari al suo valore (attento agli occhi).\newline
			Adesso se lo guardi di lato dovresti vedere un'istrogramma, nell'immagine che se avrò voglia metterò si capisce benissimo quello che intendo.\newline
			Definisco $A(\vettore v)$ l'area di quell'istogramma mostrato dalla fantomatica immagine, è facile verificare che $A(P(\vettore v,\vettore w))=\pscal{v}{w}$\footnote{supponendo che alla base dell'istogramma ci sia un quadrato di area unitaria}.



		\section{Prodotto caso continuo}
			Adesso applichiamo lo stesso ragionamento generalizzandolo con le funzioni (che come abbiamo detto sono vettori continui), in questo caso la funzione $P(\vettore v,\vettore w)(x)=\vettorec v(x)\vettorec w(x)$ e l'area è l'integrale nel dominio di definizione di $\vettore v$ e $\vettore w$, dunque
			\begin{equation}
				\pscal{v}{w}=\int_a^b \vettorec v(x)\vettorec w(x)dx
			\end{equation}
			ovviamente questo soddisfa le proprietà del prodotto scalare, non perchè lo so dimostrare, ma perchè lo so per sentito dire.\newline
			Filamente possiamo definire il prodotto matrice per vettore. Come abbiamo detto prima un prodotto tra una matrice $M$ e un vettore $\vettore v$ non è altro che un vettore $\vettore w$ che ha come componente $i$-esima il prodotto scalare canonico tra il vettore $v$ e la $i$-esima riga di $M$, quindi il prototto $\vettore w$ tra $M$ e $\vettore v$ è
			\begin{equation}
				\vettorec w(y)=\int_a^b M(x,y)\vettorec v(x) dx
			\end{equation}
			da qui è facile verificare che rispetta tutte le stesse proprietà che ha il prodotto matrice-vettore standard, infatti siano $M, N, O$ matrici continue, $\vettore v,\vettore w$ vettori, e $\lambda$ uno scalare, allora 
			\begin{equation}
				\lambda M\vettore v= M(\lambda \vettore v),\quad M\circ (N \circ O)= (M \circ N) \circ O,
			\end{equation}
			\[
				M\vettore v + N\vettore v=  (M+N)\vettore v,\quad M\vettore v+M\vettore w=M(\vettore v+\vettore w)
			\]
			
			Lascio al lettore l'esercizio di dimostrare queste poprietà, trovare la matrice identità e la formula per prodotto tra matrici\footnote{comunque se scrolli dovresti trovarle la soluzione}
		


		\section{Combinazioni lineari}
		\label{sec:comb_lin}
			In algebra lineare sappiamo che se $\vettore w\in V$ e $\{\vettore v_1,\dots,\vettore v_n\}$ è una base di $V$ abbiamo che esistono dei coefficenti $\vettorec a_1,\dots,\vettorec a_n$ tali che $\vettore w=\vettorec a_1 \vettore v_1+\dots +\vettorec a_n \vettore v_n$, scritto in forma vettoriale
			\begin{equation}
				\begin{vmatrix}
					\vettorec w_1\\
					\vdots\\
					\vettorec w_n
				\end{vmatrix}
				=
				\begin{vmatrix}
					& & \\
					\vettore v_1 & \dots & \vettore v_n\\
					& & 
				\end{vmatrix}
				\begin{vmatrix}
					\vettorec a_1\\
					\vdots\\
					\vettorec a_n
				\end{vmatrix}
			\end{equation}
			dove la matrice nel mezzo è la matrice che ha come colonne i vettori $\vettore v_1,\dots,\vettore v_n$, quindi un modo per calcolare i coefficenti $\vettorec a_1,\dots,\vettorec a_n$ è quello di calcolare l'inversa della matrice\footnote{che esiste perchè le righe sono lienarmente indipendenti} e moltiplicare membro a membro.\newline

			La stessa cosa si può fare con le matrici continue, ma prima parliamo di matrici inverse.\newline
			Sarei tentato di definire la matrice $M^{-1}$ l'inversa di $M$ se Id$(x,y)=\delta (y-x)=\int_a^b M^{-1}(x,t) M(t,y) dt$, ma per ora la definisco così:\newline
			\begin{definizione}[Matrice inversa]
				Sia $M:[a,b]\times [a,b]\rightarrow \rm I\!R$, e $N$ pure (se c'hai voglia te lo generalizzi tu), allora $N$ è l'inversa di $M$ se
				\begin{equation}
					\vettore v=NM\vettore v=MN\vettore v
				\end{equation}
			\end{definizione}
			Adesso che sappiamo cos'è l'inversa possiamo usare lo stesso ragionamento della scorsa sezione: Supponiamo che $\{M(x,y_0)\,|\, y_0\in \rm I\!R \}$ sia una base di uno spazio vettoriale $V$, allora per ogni $v\in V$ è possibile trovare una combinazione lineare dei vettori di base che sia uguale a $v$ stesso, cioè
			\begin{equation}
			\label{combinazione lineare matrice}
				\begin{vmatrix}
					\\
					\vettorec v(x)\\
					\quad
				\end{vmatrix}
				=
				\begin{vmatrix}
					\, & & \, \\
					& M(x,y) & \\
					& & 
				\end{vmatrix}
				\begin{vmatrix}
					\\
					\vettorec a(y)\\
					\quad
				\end{vmatrix}
			\end{equation}
			Di conseguenza se si vuole trovare $\vettore a$ basta moltiplicare per l'inversa entrambi i lati.



		\section{Serie di Furiè \protect\footnote{se sai come funziona puoi saltarlo, e comunque fa più confondere che altro}}
			Chi ha un'occhio attento avrà già notato che la trasformata di fourier è un prodotto matrice-vettore, ma prima di spiegare per bene il motivo è meglio cominciare dalla sua sorellina minore: la serie di fourier\newline 

			Partiamo definendo un prodotto hermitiano che ci faccia comodo
			\begin{equation}
				\pscal{v}{w}=\frac{2}{T}\int_{-\frac{T}{2}}^\frac{T}{2} \overline{\vettorec v(x)}\vettorec w(x)dx
			\end{equation}
			se prendiamo questo insieme di vettori $E_T=\{e^{i2\pi n/ T} \, | \, n \in \rm I\!\! N\}$ abbiamo che sono vettori ortonormali rispetto a questo prodotto scalare, quindi sono perfetti per usarli come base dello spazio vettoriale $C\big[-\frac{T}{2},\frac{T}{2}\big]$\footnote{a dire il vero bisognerebbe dimostrare che $Span\{E_T\}$ sia effettivamente $C\big[-\frac{T}{2},\frac{T}{2}\big]$}, o per le funzioni che oscillano con un periodo $T$.\newline
			Tutto ciò ci porta alla celeberrima serie di fouriè, dove $\widehat{\vettore e_n}=e^{i2\pi n/ T}$
			\begin{equation}
			\label{serieFuriè}
				\vettorec v(x)=\textrm{Re}\Bigg[\sum_{n=0}^{+\infty} \pscal{\widehat{e_n}}{v}\widehat{\vettore e_n}\Bigg]
			\end{equation}
			Scritto in forma vettoriale assume una forma strana, è una matrice semi continua, che sarebbe $M:[a,b]\times \{1,\dots,n\}\rightarrow \rm I\!R$
			\begin{equation}
				\begin{vmatrix}
					\,\\
					\,\\
					\vettorec v(x)\\
					\,\\
					\,
				\end{vmatrix}
				=\textrm{Re}
				\begin{vmatrix}
					\, & & &\\
					\, & & &\\
					\widehat{\vettore e_0}& \dots & \widehat{\vettore e_n}&\dots\\
					\, & & &\\
					\, & & &
				\end{vmatrix}
				\begin{vmatrix}
					\pscal{v}{\widehat{e_0}}\\
					\vdots\\
					\pscal{v}{\widehat{e_n}}\\
					\vdots
				\end{vmatrix}
			\end{equation}
			So che è un pò complicato, ma se avete dubbi chiedetemi di persona\footnote{il mio numero è +39 345 7011798}




		\section{Trasformata di Furiè}
			Ora che abbiamo reso incomprenzibile la serie, facciamo lo stesso lavoro con la trasformata.\newline \newline
			La trasformata di fourier, può essere vista come una serie di forurier con il periodo che tende a infinito, ora però c'è il problema di scegliere una base.\newline
			Non essendoci stavolta un valido motivo per escludere le funzioni periodiche che hanno la frequenza che non siano un multiplo di $1/T$ possiamo prenderci la libertà di prenderle tutte. Di conseguenza segliamo come base $B=\{\vettore f(k)=e^{ixk}=F(x,k)\,|\,k \in \rm I\!R \}$.\newline
			Come abbiamo visto nell'equazione \ref{combinazione lineare matrice} possiamo scrivere una generica funzione in questo modo
			\begin{equation}
				\begin{vmatrix}
					\\
					\vettorec v(x)\\
					\quad
				\end{vmatrix}
				=
				\begin{vmatrix}
					\, & & \, \\
					& F(x,k) & \\
					& & 
				\end{vmatrix}
				\begin{vmatrix}
					\\
					\hat{\vettorec v}(k)\\
					\quad
				\end{vmatrix}
			\end{equation}
			La quale scritta in forma esplicita diventa la trasformata di Fourier
			\begin{equation}
			\vettorec v(x)=\int_{-\infty}^{+\infty} e^{ixk}\hat{\vettorec v}(k) dy
			\end{equation}
			Da qui si evince cosa rappresenta fisicamente $\hat{\vettore v}$: essa è la funzione che indica i coefficienti da mettere davanti i vettori di base per ottenere il vettore desiderato.\newline

			Se vogliamo ottenere $\vettore{\hat v}$ bisogna moltiplicare entrambi i membri per l'inversa $F^{-1}$
			\begin{equation}
				\vettore{\hat v}=F^{-1}\vettore{v}
			\end{equation}
			Di conseguenza ci tocca trovare l'inversa di $F$ per trovare $\vettore{\hat v}$.\newline
			Visto che le funzioni trigonometriche sono ortogonali tra di loro in un intervallo finito, vogliamo vedere questa proprietà è anche vera su tutta la retta reale, di conseguenza ipotizziamo che $F^{-1}(k,x)t=\lambda e^{ikx}$ dove $\lambda$ è uno scalare.
			Adesso vediamo le proprietà del prodotto di queste matrici e vediamo che riusciamo a tirarci fuori.
			\begin{equation}
			\label{eq:tras_fourie_1}
				\hat I(k,\overline k)=F^{-1}F=\lambda\int_{-\infty}^{\infty}e^{ikx}e^{i\overline k x}dx=\lambda\int_{-\infty}^{\infty}e^{i(k+\overline k)x}dx
			\end{equation}
			Se facciamo l'integrale come se fosse una normale esponenziale otteniamo 
			\[
				F^{-1}F=-\frac{ie^{i(k+\overline k)x}}{k + \overline k}\bigg|_{-\infty}^{+\infty}	
			\]
			chiaramente l'equazione qui sopra non è ben determinata, quindi ci tocca usare qualche trucchetto.\newline
			Visto che le esponenziali complesse causano problemi all'infinito proviamo a moltiplicargi una gaussiana molto larga, cioè $e^{ikx}\rightarrow \lim_{a\to 0}e^{ikx}e^{-\frac{ax^2}{2}}$, così facendo la penultima equazione (\ref{eq:tras_fourie_1}) diventa
			\begin{equation}
			\begin{split}
				\hat I(k,\overline k)=&\lambda\int_{-\infty}^{\infty}e^{i(k+\overline k)x}dx=\lim_{a\to 0}\lambda\int_{-\infty}^{\infty}e^{i(k+\overline k)x}e^{-ax^2}dx=\\
				=&\lim_{a\to 0}\lambda\int_{-\infty}^{\infty}\exp\Bigg[-ax^2 +i(k+\overline k)x+\frac{(k+\overline k)^2}{4a}-\frac{(k+\overline k)^2}{4a}\Bigg]dx=\\
				=&\lim_{a\to 0}\lambda e^\frac{(k+\overline k)^2}{4a}
				\int_{-\infty}^{\infty}\exp\Bigg[i\sqrt ax+\frac{(k+\overline k)}{2\sqrt a}\Bigg]^2dx=\\
				=&\lim_{a\to 0}\lambda e^\frac{(k+\overline k)^2}{4a}
				\int_{-\infty}^{\infty}\exp-\Bigg[\sqrt ax-i\frac{(k+\overline k)}{2\sqrt a}\Bigg]^2dx=\\
				=&\lim_{a\to 0}\frac{\lambda}{\sqrt a} e^\frac{(k+\overline k)^2}{4a}
				\int_{-\infty}^{\infty}e^{-z^2}dz=\, \Bigg|\textrm{ ho sostituito } z=\sqrt ax-i\frac{(k+\overline k)}{2\sqrt a}\\
				=&\lambda \pi \lim_{a\to 0}\frac{e^\frac{(k+\overline k)^2}{4a}}{\sqrt{a\pi}}=\qquad \Bigg|\textrm{ sostituisco } \sigma^2=2a\\
				=& \sqrt{2\pi}\lambda\lim_{\sigma \to 0}\frac{1}{\sigma}e^{-\frac{1}{2}\big(\frac{k+\overline k}{\sigma}\big)^2}=
				2\pi\lambda\lim_{\sigma \to 0}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{k+\overline k}{\sigma}\big)^2}=\\
				=&2\pi\lambda \delta(k+\overline k)\quad \Bigg| \textrm{ Ho fatto il limite della Gaussiana a varianza nulla}
			\end{split}
			\end{equation}
			Dopo tutti questi conti otteniamo che $\hat I(-k,\overline k)=\textrm{Id}(k,\overline k)$ per $\lambda=\frac{1}{2\pi}$. quindi
			\begin{equation}
				F^{-1}(k,x)=\frac{1}{2\pi}e^{-ikx}\quad \Bigg| \quad\delta(k-\overline k)=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{i(k-\overline k)x}dx
			\end{equation}




		\section{Matrice associata a un'applicazione lineare}
			Per costruire una matrice associata a un'applicazione lineare il procedimento è pressocchè identico a quello adottato in algebra lineare:\newline
			Sia $L:V\rightarrow W$ un'applicazione lineare tra due spazii vettoriali e sia $B=\{\vettore v(y_0)|y_0\in \rm I\!R\}$ una base di $V$. Allora la matrice $M$ associata all'applicazione lineare rispetto a $B$ sarà $M(x,y)=L(\vettore v(y))(x)$.\newline
			Detto in parole povere ho messo in ogni colonna dove va a finire il vettore di base associato.\newline



		\section{Cambiamento di base}
			Trovare la matrice associata all'applicazione lineare ci dà informazioni su come trasformano i vettori di base, ma non ci dice molto su come trasforma una funzione generica.\newline
			Sia $B=\{\vettorec v(x,y_0)|y_0\in \rm I\!R\}$ una base di uno spazio vettoriare $V$ denso in $\rm I\!L$, sia $M$ la matrice assiociata all'applicazione lineare rispetto alla base $B$ e sia $N(x,y)=\vettorec v(x,y)$ la matrice di cambiamento di base, allora la matrice associata all'applicazione lineare $L$ in $V$ sarà
			\begin{equation}
				L=N^{-1}\circ M \circ N
			\end{equation}
			Che è praticamente identica alla sua controparte discreta di algebra lineare



		\section{Proprietà $\delta$ di Dirac}
			Prima di ricavarci la matrice associata alla derivata occorre sapere un paio di proprietà sulla $\delta$ di dirac.\newline
			La prima è sapere qual'è la sua derivata, che nonostante sia un pò un'abuso di notazione fa il suo sporco lavoro. Partiamo dall'osservare che $x\delta (x)=0$ per ogni $x$, adesso calcoliamoci la sua derivata.
			\[
				(x\delta (x))'=0= x\delta'(x) + \delta(x)
			\]
			risolvendo per $\delta'(x)$ si ottiene che 
			\begin{equation}
				\delta'(x)=-\frac{\delta(x)}{x}
			\end{equation}
			La seconda proprietà non la dimostro, essa afferma che
			\begin{equation}
				\delta(x-y)=\frac{1}{2\pi} \int_{-\infty}^{+\infty} e^{it(x-y)} dt
			\end{equation}



		\section{Matrice associata alla derivata}
			A questo punto possiamo trovare la matrice associata alla più famosa delle applicazioni lineare: la derivata\footnote{Se ti senti particolarmente intelligente puoi provare a trovare la matrice associata all'integrale}.\newline
			Cominciamo mettendoci nella base $B=\{e^{ixy_0}\,|\,y_0 \in \rm I\!R \}$ delle esponenziali complesse, la matrice associata alla derivazione rispetto a $x$ $\widehat{D}$ in quella base è 
			\begin{equation}
				\widehat{D}(x,y)=-iy\delta (x-y)=-ix\delta(x-y)=-i\sqrt{|xy|}\delta(x-y)
			\end{equation}
			Mentre la matrice di cambiamento di base è $F(x,y)=\frac{e^{-iyx}}{\sqrt{2\pi}}$, e la sua inversa è $F^{-1}(x,y)=\frac{e^{iyx}}{\sqrt{2\pi}}$, per avere l'applicazione derivata $D$ rispetto alla base canonica bisognerà comporre le matrici
			\begin{equation}
				D=F^{-1}\circ \widehat{D}\circ F
			\end{equation}
			Essendo la composizione di matrici associativa, conviene calcolarci prima $\widehat{D}F$
			\[
				(\widehat{D}\circ F)(x,y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}
				-iy\delta(y-t)e^{-itx}dt=-\frac{iy}{\sqrt{2\pi}}e^{-iyx}
			\]
			adesso basta moltiplicare la nuova matrice per $F^{-1}$
			\[
				(F^{-1}\circ \widehat{D}\circ F)=
				\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}-\frac{it}{\sqrt{2\pi}}e^{itx}e^{-ity}dt=
				-\frac{i}{2\pi}\int_{-\infty}^{+\infty}te^{it(x-y)}dt
			\]
			integrando per parti si ottine
			\[
				-\frac{it}{2\pi}(2\pi\delta(x-y))\bigg\rvert_{-\infty}^{+\infty}+
				\frac{i}{2\pi}\int_{-\infty}^{+\infty}\frac{e^{it(x-y)}}{i(x-y)}dt=
				\frac{1}{2\pi(x-y)}\int_{-\infty}^{+\infty}e^{it(x-y)}dt=\frac{\delta(x-y)}{x-y}
			\]
			E così abbiamo la matrice associata alla derivata nella base canonica delle funzioni
			\begin{equation}
				D(x,y)=\frac{\delta(x-y)}{x-y}
			\end{equation}
			Adesso dimostriamo che effettivamente funziona
			\[
				\int_{-\infty}^{+\infty}\frac{\delta(x-y)}{x-y}\vettorec v(y)dy=-\delta(x-y)\vettorec v(y)\big\rvert_{-\infty}^{+\infty}+\int_{-\infty}^{+\infty}\delta(x-y)\vettorec v'(y)dy=\vettorec v'(x)
			\]



		\section{Matrice associata alla derivata n-esima}
			Ora che avete visto la matrice associata alla derivata, vediamo se si può fare la stessa cosa per la derivata n-esima.
			Nella base delle esponenziali complesse la matrice che cerchiamo è 
			\begin{equation}
				\widehat{D_n}(x,y)=(-iy)^n\delta(x-y)
			\end{equation}
			A questo punto effettuiamo il cambiamento di base di quest'altra matrice
			\[
				(\widehat{D_n}\circ F)(x,y)=
				\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}(-iy)^n\delta(y-t)e^{-itx}dt=
				\frac{(-iy)^n}{\sqrt{2\pi}}e^{-iyx}
			\]
			\[
				D_n=(F^{-1}\circ \widehat{D_n}\circ F)=
				\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}\frac{(-it)^n}{\sqrt{2\pi}}e^{itx}e^{-ity}dt=
				\frac{(-i)^n}{2\pi}\int_{-\infty}^{+\infty}t^n e^{it(x-y)}dt
			\]
			Integrando per parti si ottiene che
			\[
				\frac{(-it)^n}{2\pi}(2\pi\delta(x-y))\bigg\rvert_{-\infty}^{+\infty}-
				\frac{(-i)^n}{2\pi}\int_{-\infty}^{+\infty}nt^{n-1}\frac{e^{it(x-y)}}{i(x-y)}dt=
				\frac{n(-i)^{n-1}}{2\pi(x-y)}\int_{-\infty}^{+\infty}t^{n-1}e^{it(x-y)}dt=	
			\]
			Riscrivendo l'ultima equazione in termini di $D_{n-1}$ e applicandola ricorsivamente si ottiene la formula della marice associata alla derivata n-esima\footnote{Sinceramente non è che mi convinci molto questo risultato, ma per ora farò finta che funzioni. Se riuscite a trovarmi un contr'esempio, o una formula "più corrretta" mi fate un favore}
			\begin{equation}
				D_{n}=\frac{n}{x-y}D_{n-1}=n!\frac{\delta(x-y)}{(x-y)^n}
			\end{equation}	



		\section{Risoluzione equazioni differenziali linearii a coefficenti costanti}
			Se si vuole trovare la matrice associata ad un'equazione differenziale lineare basta prenderne una a caso $a_n\vettorec v^{n}(x)+\dots +a_0\vettorec v(x)=\vettorec w(x)$ e scriverla in forma matriciale $a_nD_n\vettore v+\dots+a_0D_0\vettore v=\vettore w$.\newline
			Se definiamo $L=a_nD_n+\dots+a_0D_0$ si ottiene che $L$ è la matrice associata alla nostra equazione differenziale $L\vettore v=\vettore w$.\newline
			Di conseguenza abbiamo davanti a noi un problema che è deltutto analogo alla risoluzione di un sistema di equazioni lineri in algebra lineare $M\vettore v=\vettore w$.\newline
			Quindi la soluzione più generica sarebbe il nucleo $N$ di $L$ sommato alla soluzione dell'inversa ristretta all'immagine $I$ di $L^{-1}_{|I}\vettore g=\vettore f$.\newline

			A prima lettura sembra un problema veramente complicato, tuttavia bisogna ricordarsi che nel caso in cui si ci trova in una base di autovettori di $D_i$, $L$ risulta diagonale, e la cosa bella delle matrici diagonali è che il nucleo sono gli zeri sulla diagonale e che l'iversa è l'inverso degli elementi sulla diagonale.\newline
			Visto che le esponenziali sono una base\footnote{A dire il vero dovrei dimostrarlo, ma per ora fidati, comunque se ti interessa perchè, vatti a studiare la trasformata di Laplace. In ogni caso lo stesso identico risultato si sarebbe ottenuto con le esponenziali complesse, ma sarebbe stato più fastidioso da ricavare a causa dei numeri complessi} di autovettori rispetto alla derivata conviene risolvere il probema rispetto ad essa.\newline

			Rispetto alla base $B=\{e^{yx}|y \in \rm I\!R\}$ $D_n=y^n\delta(x-y)$, di conseguenza $L(x,y)=\delta(x-y)(a_ny^n+\dots+a_0)$, quindi gli zeri lungo la diagonale $y_0,\dots,y_n$ sono le soluzioni di $P(y)=a_ny^n+\dots+a_0$, inoltre per il teorema fondamentale dell'algebra si ha che $dim(N)=n$. Il nucleo sarà quindi nella forma $N=span\{e^{y_0x},\dots,e^{y_nx}\}$.\footnote{Se la tua domanda è:"Ma tu vu futtiri a mia?! Chi succiri si i radici su i stissi!?" ti posso soltanto dire che ci sto Travagghiando}\newline

			Per trovare l'omogenea $\vettore v_o$ bisognerà risolvere $f_o=L^{-1}_{|I}\vettore g$, ma per ora mi secca scrivere come si fa.










	\chapter{Algebra Multilineare}

		Questo capitolo conterrà diversi errori perchè toccherà diversi argomenti che non ho ancora studiato tra cui l'algebra multilineare stessa e l'analisi funzionale, quindi se ci sono inprecisioni ditemelo che correggerò.\newline
		Inoltre da questo punto in poi bisogna sapere almeno le basi sui tensori e il prodotto tensoriale, consiglio di leggere \href{url}{https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia}.

		\section{Applicazioni Multilineari e Tensori}
			Un'applicazione multilineare $M:V_1\times\dots\times V_n\rightarrow W$\footnote{Talvolta $V_1\times\dots\times V_n$ viene indicata come $\prod_i V_i$, invece $V\times\dots\times V$ si indica come $V^n$} è una funzione lineare negli argomenti, cioè tale che
			\[
				M(\vettore v_1,\dots,\vettore v_i,\dots,\vettore v_n)+M(\vettore v_1,\dots,\vettore{\hat v}_i,\dots,\vettore v_n)=
				M(\vettore v_1,\dots,\vettore v_i+\vettore{\hat v}_i,\dots,\vettore v_n)
			\]
			\begin{equation}
				M(\vettore v_1,\dots,\lambda \vettore v_i,\dots,\vettore v_n)=
				\lambda M(\vettore v_1,\dots,\vettore v_i,\dots,\vettore v_n)
			\end{equation}
			Per trovare il tensore associato ad un'applicazione multilineare bisogna esprimere ogni vettore in termini dei vettori di base del proprio spazio vettoriale.\newline
			Per semplicità prendiamo un'applicazione multinineare $M:V^n\rightarrow W$, dove Dim$(V)=m$\newline
			ogni vettore $\vettore v_i$ può essere scritto come combinazione lineare dei vei vettori di base in questo modo 
			\[
				\vettore v_j=\vettore b_1 a_{j,1}+\dots +\vettore b_m a_{j,m}=\sum_{i_j}^m\vettore b_{i_j}a_{j,i_j}
			\]
			Grazie a questo possiamo riscrivere $M(\vettore v_1,\dots,\vettore v_n)$ in un modo più complicato\newline
			\[
				M(\vettore v_1,\dots,\vettore v_n)=
				\sum_{i_1,\dots,i_n}M(\vettore b_{i_1}  a_{1,{i_1}},\dots,\vettore b_{i_n} a_{n,{i_n}})=
			\]
			\[
				=\sum_{i_1,\dots,i_n}M(\vettore b_{i_1},\dots,\vettore b_{i_n})a_{1,{i_1}}\dots a_{n,{i_n}}
			\]
			Adesso definisco il Tensore $T$
			\begin{equation}
				T_{i_1,\dots,i_n}=M(\vettore b_{i_1},\dots,\vettore b_{i_n})
			\end{equation}
			Quindi
			\begin{equation}
				M(\vettore v_1,\dots,\vettore v_n)=\sum_{i_1,\dots,i_n}T_{i_1,\dots,i_n}a_{1,{i_1}}\dots a_{n,{i_n}}
			\end{equation}
			se si sceglie come base quella canonica, allora diventa tutto più "carino"
			\[
				M(\vettore v_1,\dots,\vettore v_n)=\sum_{i_1,\dots,i_n}T_{i_1,\dots,i_n}
				(\vettore v_1)_{i_1}(\vettore v_n)_{i_n}	
			\]
			Nel caso più generico in cui $M:V_1\times\dots\times V_n\rightarrow W$ andrebbe scelta una base diversa per ogni spazio vettoriale $V_i$, concettualmente non cambia molto, ma il numero di indici aumenta, il che renderebbe ancora più confusionaria la situazione, quindi se c'avete voglia fatevelo per conto vostro.




		\section{Tensori Continui}
			Adesso possiamo studiare come cambia tutto se come vettori scegliamo dei vettori continui.\newline
			Partiamo per semplicità da un'applicazione bilineare $M:V\times V\rightarrow W$.
			Sia $\{\vettore b(x)|x \in \rm I\!R\}$ una base di $V$, allora per ogni $\vettore v\in V$ esiste una combinazione lineare di dei vettori di base che sia uguale a $\vettore v$, in matematichese si ha che $\vettore v=\int_I \vettore b(x)\vettorec a(x)dy$, dove $I$ è l'insieme d'integrazione. Quindi una applicazione bilineare tra due vettori contunui $\vettore v_1$ e $\vettore v_2$ diventa
			\begin{equation}
			\begin{split}
				M(\vettore v_1,\vettore v_2)=& M\bigg(\int_I \vettore b(x_1)\vettorec a_1(x_1)dx_1,\int_I \vettore b(x_2)\vettorec a_2(x_2)dx_2\bigg)=\\
				& =\int_I M\bigg(\vettore b(x_1),\int_I \vettore b(x_2)\vettorec a_2(x_2)dx_2\bigg)\vettorec a_1(x_1)dy_1=\\
				&=\int_I \int_I M(\vettore b(x_1),\vettore b(x_2))\vettorec a_1(x_1)\vettorec a_2(x_2)dx_1dx_2
			\end{split}
			\end{equation}
			A questo punto basta definire un tensore continuo (per le applicazioni bilineari) come quel coso così
			\[
				T(y_1,y_2)=M(\vettore b(y_1),\vettore b(y_2))
			\]
			e quindi l'equazione di prima diventa
			\[
				M(\vettore v_1,\vettore v_2)=\int \int T(y_1,y_2)\vettorec a_1(y_1)\vettorec a_2(y_2)dy_1dy_2
			\]
			\newline

			Adesso prendiamo un'applicazione lineare $M:V_1\times\dots\times V_n\rightarrow W$, siano $B_i=\{\vettore b_i(x)|x\in \rm I \! R\}$ delle basi dei rispettivi $V_i$, $\vettore v_i \in V_i$ dei vettori generici esprimibili in termini delle $B_i$ in questo modo $\vettore v_i=\int_{I_i} \vettore b_i(y)a_i(x)dx$.
			A questo punto l'applicazione multilineare diventa
			\begin{equation}
			\begin{split}
				& M(\vettore v_1,\dots,\vettore v_n)=\\
				&=M\bigg(\int_{I_1} \vettore b_1(x_1)\vettorec a_1(x_1)dx_1,\dots,\int_{I_n} \vettore b_n(x_n)\vettorec a_n(x_n)dx_n\bigg)=\\
				&=\int_{I_1}\dots\int_{I_n}M(\vettore b_1(x_1),\dots,\vettore b_n(x_n))
				\vettorec a_1(x_1)\dots\vettorec a_n(x_n)dx_1\dots dx_n
			\end{split}
			\end{equation}
			definendo $T(x_1,\dots,x_n)=M(\vettore b_1(x_1),\dots,\vettore b_n(x_n))$ si può espimere il tutto così
			\begin{equation}
			\label{eq:prod_vett_tens}
				M(\vettore v_1,\dots,\vettore v_n)=\int_{I_1}\dots\int_{I_n}T(x_1,\dots,x_n)
				\vettorec a_1(x_1)\dots\vettorec a_n(x_n)dx_1\dots dx_n
			\end{equation}
			e questi sono i Tensori Continui.




		\section{Funzionali}
			Un tensore $T$ a $n$ indici può essere visto come un oggetto che prende un vettore $n$-dimenzionale e ti sputa qualcosa fuori.
			Se facciamo il limite al continuo degli indici otteniamo che il tensore diventa della forma $T(\vettore v)$, dove $\vettore v$ è un vettore continuo.\newline
			Essi sono funzioni che hanno come dominio lo spazio delle funzioni. Questi oggetti in matematica sono definiti come Funzionali e sono indicati con $T[\vettore v]$.\newline
			Esistono diversi tipi di funzionali, ma io mi concentrerò su quelli più facili, cioè quelli tali che
			\begin{equation}
				\lim_{\epsilon 	\to 0}T[\vettore v+\epsilon \vettore w]-T[\vettore v]=0
			\end{equation}
			Che chiamerò funzionali continui\footnote{Probablilmente ce l'hanno una definizione per come si deve, ma io non la so, quindi ci metto questa perchè assomigliano alle funzioni continue di Analisi 1}.



		\section{Derivata funzionale}
			Le analogie tra le funzioni in $n$ variabili e i funzionali sono molte, una di queste è la derivata direzionale.\newline
			Nelle funzioni in più variabili la derivata direzionale è\footnote{ricordati che $||w||_2=1$} $\lim_{\epsilon \to 0}\frac{f(\vettore v+\epsilon\vettore w)-f(\vettore v)}{\epsilon}$. Nei funzionali invece di prendere $\vettore v,\vettore w$ vettori discreti, prendiamo vettori continui, e così facendo si ottene la derivata funzionale
			\begin{equation}
				\frac{\partial F}{\partial \vettore w}[\vettore v]=\lim_{\epsilon \to 0}\frac{F[\vettore v+\epsilon \vettore w]-F[\vettore v]}{\epsilon}
			\end{equation}
			con $||\vettore w||_2=1$. Questa equazione rappresenta la derivata di $F$ nella direzione di $\vettore w$.\newline
			Se si vuole trovare un punto stazionario di un funzionale $F$ bisogna imporre che $\frac{\partial F}{\partial \vettore w}=0$ per ogni $\vettore w$, che equivarrebe a dire che la derivata deve essere zero in ogni direzione.\newline



		\section{Equazioni di Eulero-Lagrange}
			L'esempio più famoso nel trovare i punti stazionari di un funzionale è l'equazione di Eulero-Lagrange:
			\begin{equation}
				A[x(t)]=\int_{t_a}^{t_b}L(x(t),\dot{x}(t),t)dt
			\end{equation}
			dove $A$ è un funzionale detto Azione che per qualche oscuro motivo dobbiamo minimizare, $L$ è la lagrangiana, $x$ è la posizione, $\dot{x}$ la velocità e $t$ il tempo.
			Per trovare il minimo bisogna imporre che per ogni $y$
			\[
				\frac{\partial A}{\partial y}=\lim_{\epsilon \to 0}\frac{A[x(t)+\epsilon y(t)]-A[x(t)]}{\epsilon}=0
			\]
			sviluppo in Taylor il primo membro
			\[
				A[x(t)+\epsilon y(t)]=\int_{t_a}^{t_b}L(x+\epsilon y,\dot{x}+\epsilon \dot{y},t)=A[x(t)]+\int_{t_a}^{t_b}\frac{\partial{L}}{\partial{x}}\epsilon y+\frac{\partial{L}}{\partial{\dot{x}}}\epsilon \dot{y}dt
			\]
			quindi
			\[
				\frac{\partial A}{\partial y}=\int_{t_a}^{t_b}\frac{\partial{L}}{\partial{x}}y+\frac{\partial{L}}{\partial{\dot{x}}}\dot{y}dt
			\]
			integrando per parti il secondo termine dell'integrale
			\[
				\int_{t_a}^{t_b}\frac{\partial{L}}{\partial{\dot{x}}}\dot{y}dt=y\frac{\partial{L}}{\partial{\dot{x}}}\bigg\rvert_{t_a}^{t_b}-
				\int_{t_a}^{t_b}\frac{d}{dt}\bigg(\frac{\partial{L}}{\partial{\dot{x}}} \bigg)ydt
			\]
			Adesso per semplicità assumiamo che i punti estremali della traiettoria sono fissati, quindi $y(t_a)=y(t_b)=0$, quindi rimettendo tutto nell'equazione precedente si ha che
			\begin{equation}
				\frac{dA}{dy}=\int_{t_a}^{t_b}\bigg[\frac{\partial{L}}{\partial{x}}-\frac{d}{dt}\bigg(\frac{\partial{L}}{\partial{\dot{x}}} \bigg)\bigg]ydt
			\end{equation}
			che è uguale a zero per ogni $y(t)$ se e solo se 
			\begin{equation}
				\frac{\partial{L}}{\partial{x}}=\frac{d}{dt}\bigg(\frac{\partial{L}}{\partial{\dot{x}}} \bigg)
			\end{equation}
		Purtroppo in generale minimizzare un funzionale non è così semplice, infatti non sono ancora riuscito a minimizzare il funzionale $N[\vettore v]=||M\vettore v||$, che permetterebbe di trovare il nucleo di $M$.




		\section{Gradiente e Sviluppo in Taylor di Funzionali}
			In una funzione $f$ in $n$ variabili il gradiente è un vettore $n$ dimenzionale tale che $\nabla f_i=\partial_{\vettore e_i} f$, dove $\vettore e_i$ è il versore $i$-esimo.\newline
			Sia $V$ lo spazio vettoriale delle funzioni $\vettore v:[a,b]\rightarrow R$, se prendiamo un funzionale $T:V\rightarrow \rm I\!R$, e definiamo $\vettore e(x_0)$ il versore associato alla "direzione" $x_0$-esima\footnote{che potrebbe essere $\frac{\delta (x-x_0)}{\delta(0)}$ (perchè $||e(x_0)||_2$ deve essere uguale a $1$), ma preferisco restare generico lasciando $e(x_0)$}, il gradiente di questo tipo di funzionale diventa
			\begin{equation}
				(\nabla T[\vettore v])(x_0)=\frac{\partial T[\vettore v]}{\partial \vettore e(x_0)}
			\end{equation}
			Quindi  il gradiente di un funzionale $\nabla T$, è a sua volta un tensore che va dallo spazio delle funzioni allo spazio dei vettori continui.\newline
			Generalizzando si ha che il tensore associato alle derivate parziali di ordine $n$-esimo è un tensore $D_n:V\times \rm I\!R^n\rightarrow \rm I\!R$
			\begin{equation}
				(D_n[\vettore v])(x_1,\dots,x_n)=\frac{\partial^n T[\vettore v]}{\partial \vettore e(x_1)\dots\partial \vettore e(x_n)}
			\end{equation}
			Grazie a ciò è possibile creare un'analogo allo sviluppo in taylor in un continuo di dimenzioni.\newline
			Come al solito per capire il limite al continuo di qualcosa può essere utile vedere prima come si descrive ciò in un numero finito di dimenzioni, in particolare lo sviluppo in Taylor di una funzione $g(\vettorec x_1,\dots,\vettorec x_n)=g(\vettore x)$ di ordine $m$ attorno al punto $(\hat{\vettorec{x}}_1,\dots,\hat{\vettorec{x}}_n)=\vettore{\hat x}$ è
			\begin{equation}
				g(\vettore{\hat x})+\sum_{i}^{n}\frac{\partial g(\vettore{\hat x})}{\partial \vettore e_i}(\vettorec x_i-\hat{\vettorec{x}}_i)+\dots+
				\sum_{i_1,\dots,i_m}^{n}
				\frac{\partial^m g(\vettore{\hat x})}
				{\partial \vettore e_{i_1}\dots\partial \vettore e_{i_m}}
				(\vettorec x_{i_1}-\hat{\vettorec{x}}_{i_m})\dots
				(\vettorec x_{i_m}-\hat{\vettorec{x}}_{i_m})
			\end{equation}
			A questo punto, per dedurre lo sviluppo in Taylor di un funzionale basta tenere conto che nel caso finito le coordinate devono andare da $1$ a $n$, quindi ogni $i_j$ assume valori in $\{1,\dots,n\}$ e poi sommare.\newline
			Nel caso continuo però gli indici $\vettorec x_j$ assumono valori in $[a,b]$, quindi bisogna sommare su tutti i valori in quell'intervallo, che equivale a integrare.\footnote{spiegazione per niente chiara, riscriverla (anche se il concetto è proprio na stronzata)}\newline
			Quindi l'equazione dello sviluppo in Taylor di un funzionale diviene
			\begin{equation}
				T[\hat f]+\int_a^b \frac{\partial T[\hat f]}{\partial e(x)}[f(x)-\hat f(x)]dx\quad+\dots+
			\end{equation}
			\[
				+\underbrace{\int_a^b\dots\int_a^b}_{n\textrm{ volte}}
				\frac{\partial^n T[\hat f]}{\partial e(x_1)\dots\partial e(x_n)}[f(x_1)-\hat f(x_1)]\dots[f(x_n)-\hat f(x_n)]dx_1\dots dx_n=
			\]
			%\begin{equation}
			%	=\sum_{i=0}^n \Bigg\{\prod_{j=1}^i \int_a^b dx_j[f(x_j)-\hat f(x_j)]\frac{\partial}{\partial e(x_j)}\Bigg\}T[\hat f]
			%\end{equation}
			che se dio vuole rispetta il teorema di Swartz\footnote{Per chi non si ricordasse è quello che dice che $\partial_x\partial_y f=\partial_y\partial_x f$}




		\section{Volume parallelogramma in un continuo di dimenzioni}
			Generalizzare il determinante è più difficile di quanto sembri, e ne parlerò a breve nel paragrafo ~\ref{sec:det} a pagina ~\pageref{sec:det}, ma prima è inutile parlare di determinante se non si sa nemmeno qual'è il limite al continuo del volume di un parallelogramma.\newline
			Visto che ci troviamo in un continuo di dimenzioni, possiamo individuare una dimenzione con un numero reale $x\in[a,b]$ e la lunghezza dello spigolo in quella direzione avrà un certo valore $\vettorec v(x)\in[0,+\infty)$.\newline
			Chiaramente se definiamo il volume come il prodotto di tutti i valori di $\vettorec v(x)$ perde ogni senzo, ma possiamo tentare di trovare qualcosa che si ci avvicini.\newline

			Supponiamo di avere un parallelepipedo $n$ dimenzionale, esso può essere descritto da un vettore $\vettore v$ che ha come elementi la lunghezza di ogni spigolo $(v_1,\dots,v_n)$, adessso usando un ragionamento simile a quello del paragrafo sul prodotto scalare canonico ~\ref{sec:prod-can} a pagina ~\pageref{sec:prod-can} possiamo visualizzare il vettore come un'istogramma che ha ogni elemento con una larghezza di base $l=1$.\newline
			Senza perdere di generalità possiamo affermare che il volume del parallelepipedo è il prodotto di ogni elemento del vettore $\prod \vettorec v_i$ elevato alla larghezza di base (che in questo caso non fa nessuna differenza).\newline

			Adesso prendiamo un parallelepipedo in un continuo di dimenzioni individuato dal vettore continuo $\vettore v$ con $\vettorec v(x)\in[0,+\infty)$ e $x\in[a,b]$ e approssimiamo il vettore continuo $\vettore v$ con uno discreto (come quando si fanno gli integrali).\newline
			Il volume del parallelepipedo così definito è uguale a
			\begin{equation}
			\label{eq:volterra}
			\begin{split}
				\Bigg[\prod_{i=0}^n \vettorec v\bigg(a+\frac{i(b-a)}{n}\bigg)\Bigg]^{\frac{b-a}{n}}&=
				\textrm{exp}\Bigg\{\sum_{i=0}^n\frac{b-a}{n}\textrm{ln}\bigg[\vettorec v\bigg(a+\frac{i(b-a)}{n}\bigg)\bigg]\Bigg\}=\\
				&=\textrm{exp}\bigg[\int_a^b \textrm{ln}\vettorec v(x)dx\bigg]
			\end{split}
			\end{equation}
			Dove dal secondo al terzo passaggio ho scritto che la somma dell'area di tutti quei rettangolini è l'integrale di quella roba.\newline
			L'ultimo integrale è detto l'Integrale di Volterra, perchè lui l'ha scoperto prima di me.
			Come abbiamo visto grazie al cielo questa roba tende a un valore ben determinato che è pure possibile calcolare. Questo procedimento può essere usato per portare una produttoria generica nel limite al continuo, così come l'integrale può essere usato per portare nel limite al continuo una sommatoria.\footnote{
				Da notare che non funziona niente se $f$ assume valori negativi, infatti non è possibile decidere se suddividere un numero pari o dispari di volte gli intervallini e quindi è impossibile determinare il segno.
				\newline Questo viene rispecchiato dal fatto che nella formula scritta come integrale la funzione deve infilarsi dentro un logaritmo, il quale ammette solo argomenti positivi.
				\newline Comunque è sempre possibile prendere il valore assoluto di $f$ e buona notte.\newline

				Inoltre c'è da dire che questo non é veramente il volume di un parallelogramma in infinite dimenzioni, è semplicemente una definizione che gli ho dato perchè così ha parecchi bonus: è semplice da calcolare, è intuitivo (almeno per me), e il volume di $\vettorec v(x)=1$ calcolato tra $0$ e $1$ fa 1. Ok, devo ammettere che l'ultima non è proprio convincente...\newline}




		\section{Come trasforma un Funzionale}
			Fin'ora abbiamo analizzato i funzionali senza moltiplicarli tra di loro, ma adesso che abbiamo uno strumento che ci manda al continuo le produttorie possiamo vedere come trasforma l'equazione del prodotto vettori-tensore\footnote{l'equazione "originale" è la ~\ref{eq:prod_vett_tens} a pag \pageref{eq:prod_vett_tens}} quando il tensore divena un funzionale.\newline
			Partiamo scrivendo l'equazione del prodotto tensore-vettori.
			\begin{equation}
				\int T(\vettorec v_1,\dots,\vettorec v_n)a_1(\vettorec v_1)\dots a_n(\vettorec v_n)d\vettorec v_1\dots d\vettorec v_n=
			\end{equation}
			\[
				=\int T(\vettorec v_1,\dots,\vettorec v_n)\prod_{x=1}^n a_x(\vettorec v_x)d\vettorec v_x
			\]
			adesso se mandiamo $n$ al continuo la $x$ diventa una variabile reale, quindi il tensore diventa un funzionale $T(\vettorec v_1,\dots,\vettorec v_n)\rightarrow T[\vettore v]$, il differenziale $\prod d\vettorec v_x$ passa da essere un differenziale in $\rm I\!R^n$ a essere un differenziale nello spazio delle funzioni $d[\vettore v]$;\newline
			Tutte le cose che avevano una $x$ al pedice gli passa all'argomento $a_x\rightarrow a(x)$, $\vettorec v_x\rightarrow \vettore v(x)$, $a_x(\vettorec v_x)\rightarrow a(x)[\vettorec v(x)]$\footnote{Non so quanto senso abbia mettere una $x$ sia fuori che dentro il funzionale, intanto io ce lo lascio, male che vada basta scrivere solo $a[\vettorec v(x)]$ o $a(x)[\vettore v]$. Sinceramente credo che la seconda opzione sia quella più corretta, ma intanto la lascio scritta così com'era, non si sa mai}; e infine la produttoria diventa un integrale sfruttando l'equazione ~\ref{eq:volterra} del paragrafo di prima 
			\[
				\prod_{x=1}^n a_x(\vettorec v_x)\rightarrow\textrm{exp}\bigg[\int \textrm{ln}\big( a(x)[\vettorec v(x)]\big)dx\bigg]
			\]
			infine tutto diviene
			\begin{equation}
				\int T[\vettore v]\textrm{exp}\bigg[\int \textrm{ln}\big(a(x)[\vettorec v(x)]\big)dx\bigg]d[\vettore v]
			\end{equation}
			Sinceramente non so quanto sia calcolabile un integrale nello spazio delle funzioni, credo che per andare avanti servano conoscenze che al momento non ho, oppure c'è qualcosa che mi manca.\newline
			Comunque questa roba credo che ce l'abbia un'applicazione in fisica, infatti l'integrale dei cammini di Feynmann ha una forma simile, ma preferisco non sbilanciarmi visto che di teoria quantistica dei campi non ne so niente




		\section{Determinante}
		\label{sec:det}
			Il determinante che conosciamo tutti è un'applicazione mulilineare antisimmetrica rispetto ai vettori colonna tale che Det$(\vettore e_1,\vettore e_2,\dots,\vettore e_n)=1$.\newline
			Riuscire a trovare il tensore associato $\epsilon_{i_1,\dots,i_n}=$Det$(\vettore e_{i_1},\dots,\vettore e_{i_n})$ è abbastanza semplice.\newline
			L'antisimmetria ci assicura che se due incici si ripetono allora il determinante è nullo $\epsilon_{i_1,\dots,i_a,\dots,i_a,\dots,i_n}=-\epsilon_{i_1,\dots,i_a,\dots,i_a,\dots,i_n}=0$.\footnote{questo è in accordo con il fatto che se dei vettori sono linearmente dipendenti allora il determinante si annulla}\newline
			Un'altra importante proprierà derivante dall'antisimmetria è che un numero pari di permutazioni di indici scambiati a due a due lascia il tensore invariato, mentre un numero dispari lo cambia di segno.\footnote{questo è in accordo col fatto che se scambi due colonne il determinante cambia di segno}\newline
			Usando il fatto che $\epsilon_{1,2,\dots,n}=1$ si ottiene che il tensore associato al detetermiante è la generalizzazione in $n$ indici del tensore di Levi-Civita.\newline

			Mettendo tutto insieme si ottiene la formula tensoriale del deteminante
			\begin{equation}
				\textrm{Det}(\vettore v_1,\dots,\vettore v_n)=\epsilon_{i_1,\dots,i_n}(\vettore v_1)_{i_1}\dots (\vettore v_n)_{i_n}
			\end{equation}
			Purtroppo però è impossibile generalizzare al continuo il tensore di Levi-Civita visto che i funzionali completamente antisimmetrici soffrono di crisi esistenziali.\newline
			Infatti se prendo una funzione $\vettore v:[a,b]\in\rm I\!R$ e mi calcolo $T[\vettore v]$ (con $T$ antisimmetrico) questo mi restituirà un qualche valore, ma se scambio due intervalli del dominio di $\vettore v$ è impossibile stabilire se il numero di permutazioni è pari o dispari rendendo impossibile capire se l'applicazione cambia di segno oppure no.






\end{document}